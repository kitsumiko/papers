# AI/ML Research Papers Collection

> A curated, pedagogically-organized collection of essential research papers spanning the landscape of modern artificial intelligence and machine learning.

[![Papers](https://img.shields.io/badge/papers-102+-blue.svg)](by-date.md)
[![Learning Path](https://img.shields.io/badge/learning-13_phases-green.svg)](learning-path.md)
[![Glossary](https://img.shields.io/badge/glossary-250+_terms-purple.svg)](learning/glossary.md)
[![Last Updated](https://img.shields.io/badge/updated-October_30,_2025-orange.svg)](by-date.md)

---

## 🎯 What's Inside

This repository contains **102+ carefully selected research papers** organized in three complementary ways:

1. **📚 [Structured Learning Path](learning-path.md)** - A 13-phase curriculum designed to take you from foundations to cutting-edge research
2. **📅 [Chronological Timeline](by-date.md)** - Papers organized by publication date (1997-2025)
3. **📖 [Comprehensive Glossary](learning/glossary.md)** - 250+ terms, concepts, and acronyms explained with context

**Plus**: [Recommended reading strategies](#-how-to-use-this-repository), [notes on learning](#paper-reading-tips), and [quick start guides](#-quick-start) tailored to your role (beginner, practitioner, researcher, engineer, security specialist).

### Why This Collection?

- **Curated for Learning**: Papers selected for their pedagogical value and impact
- **Pedagogically Organized**: Follow a structured path from basics to advanced topics
- **Modern & Comprehensive**: Covers transformers, LLMs, agents, vision, security, and more
- **Glossary Integrated**: Key terms linked throughout—never get lost in jargon
- **Open Access Focused**: Most papers freely available; paywalled papers marked 🔒
- **Actively Maintained**: Updated with latest research from 2025

---

## 🚀 Quick Start

### For Beginners
Start with the **[Learning Path](learning-path.md)** and follow **Phase 1: Foundations**. Read papers in sequence, focusing on the "Why" explanations. Check the **[Glossary](learning/glossary.md)** whenever you encounter unfamiliar terms.

### For Practitioners
Jump to relevant phases:
- **LLMs & Training**: [Phase 2](learning/phase-02-llms.md)
- **Efficient Models**: [Phase 3](learning/phase-03-attention.md)
- **Production AI**: [Phase 4](learning/phase-04-retrieval.md) (RAG), [Phase 8](learning/phase-08-security.md) (Security)

### For Researchers
Browse the **[Chronological View](by-date.md)** to see latest 2025 research, or deep-dive into:
- [Phase 6: Alternative Architectures](learning/phase-06-architectures.md)
- [Phase 7: Interpretability](learning/phase-07-interpretability.md)
- [Phase 9: Advanced Topics](learning/phase-09-advanced.md)

### Quick Reference
**Need a definition?** → Check the **[📖 Glossary](learning/glossary.md)** for 250+ terms organized by category (architectures, training, NLP, security, etc.)

---

## 🎓 How to Use This Repository

### Reading Strategies

**🌱 The Beginner Path** (3-6 months)
1. Start with [Phase 1: Foundations](learning/phase-01-foundations.md)
2. Read key papers: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) → [BERT](https://arxiv.org/abs/1810.04805) → [GPT-3](https://arxiv.org/abs/2005.14165)
3. Focus on "Why" explanations before diving deep
4. Take notes on connections between papers

**⚡ The Practitioner Sprint** (1-2 months)
1. Read Phase 1 summaries for context
2. Deep-dive: [Phase 2](learning/phase-02-llms.md) + [Phase 3](learning/phase-03-attention.md) + [Phase 4](learning/phase-04-retrieval.md)
3. Skim related work sections to understand landscape
4. Implement key techniques from papers

**🔬 The Researcher Deep-Dive** (Ongoing)
1. Use [chronological view](by-date.md) for latest research
2. Focus on specific phases relevant to your research
3. Read citations and follow paper connections
4. Compare approaches across different papers

**🛠️ The Engineer Focus** (2-4 weeks)
1. Priority: [Phase 3](learning/phase-03-attention.md) (Efficiency), [Phase 8](learning/phase-08-security.md) (Security), [Phase 12](learning/phase-12-hardware.md) (Hardware)
2. Focus on implementation details and benchmarks
3. Note production considerations and trade-offs

**� The Security Specialist** (1-2 weeks)
1. Core: [Phase 8](learning/phase-08-security.md)
2. Context: [Phase 2](learning/phase-02-llms.md) (LLM basics), [Phase 5](learning/phase-05-reasoning.md) (Alignment)
3. Focus on threat models and defense mechanisms

### Paper Reading Tips

1. **Start with abstracts** - Understand the core contribution
2. **Read "Why" annotations** - Context before content
3. **Check the [📖 Glossary](learning/glossary.md)** - Look up unfamiliar terms
4. **Follow the narrative** - Papers build on each other
5. **Take notes** - Document connections and insights
6. **Implement key ideas** - Hands-on learning reinforces concepts

---

## 📖 Learning Path Overview

The learning path is organized into **13 progressive phases**, each building on previous knowledge:

| Phase | Topic | Papers | Focus |
|-------|-------|--------|-------|
| **[1](learning/phase-01-foundations.md)** | 🏗️ **Foundations** | 15 | Deep learning basics, embeddings, CNNs, RNNs, GANs, tokenization |
| **[2](learning/phase-02-llms.md)** | 🤖 **Large Language Models** | 10 | Transformers, BERT, GPT, training at scale |
| **[3](learning/phase-03-attention.md)** | ⚡ **Attention Innovations** | 7 | FlashAttention, efficient attention, long context |
| **[4](learning/phase-04-retrieval.md)** | 🔍 **Retrieval & RAG** | 6 | RAG systems, kNN-LM, semantic search |
| **[5](learning/phase-05-reasoning.md)** | 🧠 **Reasoning & Agents** | 12 | RLHF, chain-of-thought, agentic systems |
| **[6](learning/phase-06-architectures.md)** | 🏛️ **Alternative Architectures** | 7 | RWKV, Mamba, state-space models, theory |
| **[7](learning/phase-07-interpretability.md)** | 🔬 **Interpretability** | 10 | LIME, integrated gradients, evaluation methods |
| **[8](learning/phase-08-security.md)** | 🛡️ **Security & Robustness** | 7 | Alignment, jailbreaking, adversarial ML |
| **[9](learning/phase-09-advanced.md)** | 🎯 **Advanced Applications** | 7 | Multimodal, scientific AI, test-time compute |
| **[10](learning/phase-10-probabilistic.md)** | 🎲 **Probabilistic Models** | 4 | Diffusion, probabilistic programming |
| **[11](learning/phase-11-vision.md)** | 👁️ **Vision & Multimodal** | 8 | ViT, CLIP, SAM, vision-language models |
| **[12](learning/phase-12-hardware.md)** | ⚙️ **Hardware & Systems** | 2 | Photonic computing, VLSI implementations |
| **[13](learning/phase-13-policy.md)** | 📜 **Policy & Governance** | 1 | AI safety, policy, societal impact |

**Total**: 96+ core papers across 13 phases

---

## 🗂️ Repository Structure

```
papers/
├── README.md                    # This file - repository overview
├── learning-path.md             # Main learning path navigation
├── by-date.md                   # Chronological paper listing
└── learning/                    # Phase-by-phase curriculum
    ├── glossary.md              # Comprehensive glossary of terms & concepts
    ├── phase-01-foundations.md
    ├── phase-02-llms.md
    ├── phase-03-attention.md
    ├── phase-04-retrieval.md
    ├── phase-05-reasoning.md
    ├── phase-06-architectures.md
    ├── phase-07-interpretability.md
    ├── phase-08-security.md
    ├── phase-09-advanced.md
    ├── phase-10-probabilistic.md
    ├── phase-11-vision.md
    ├── phase-12-hardware.md
    └── phase-13-policy.md
```

---

## 📊 Coverage by Topic

This collection spans the full spectrum of modern AI/ML research, organized by theme:

### 🏗️ Deep Learning Foundations
- Classic architectures: [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), [AlexNet](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), [GoogLeNet](https://arxiv.org/abs/1409.4842), [ResNet](https://arxiv.org/abs/1512.03385)
- Training techniques: [Xavier initialization](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), [Batch Normalization](https://arxiv.org/abs/1502.03167)
- **Word Embeddings**: [Word2Vec](https://arxiv.org/abs/1301.3781), [GloVe](https://aclanthology.org/D14-1162/), [Seq2Seq](https://arxiv.org/abs/1409.3215) encoder-decoder architectures
- Sequence models: [LSTM](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) 🔒, time-series foundations ([MOMENT](https://arxiv.org/pdf/2402.03885))
- Generative models: [GANs](https://arxiv.org/abs/1406.2661), diffusion models
- **Tokenization**: [BPE](https://arxiv.org/abs/1508.07909), [SentencePiece](https://arxiv.org/abs/1808.06226), [tokenization bias](https://arxiv.org/abs/2406.16829) and [best practices](https://arxiv.org/abs/2403.01289)

### 🤖 Large Language Models & Transformers
- **Foundational**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762), [BERT](https://arxiv.org/abs/1810.04805), [GPT-3](https://arxiv.org/abs/2005.14165)
- **Modern LLMs**: [Llama 3](https://arxiv.org/abs/2407.21783), [OpenELM](https://arxiv.org/pdf/2404.14619), [EuroLLM](https://arxiv.org/pdf/2409.11741)
- **Training at Scale**: [Megatron-LM](https://arxiv.org/abs/1909.08053), [ZeRO](https://arxiv.org/abs/1910.02054), [model parallelism](https://arxiv.org/abs/2104.04473), [second-order optimization](https://arxiv.org/pdf/2510.09378)
- **Efficiency**: [MatMul-free models](https://arxiv.org/pdf/2406.02528), [1-bit LLMs](https://arxiv.org/pdf/2402.17764), [vocabulary optimization](https://arxiv.org/abs/2411.09009)

### ⚡ Efficient Attention & Long Context
- FlashAttention: [IO-aware attention algorithms](https://arxiv.org/abs/2205.14135)
- Alternative mechanisms: [RetNet](https://arxiv.org/abs/2307.08621), [attention sinks](https://arxiv.org/pdf/2309.17453.pdf), [Infini-attention](https://arxiv.org/pdf/2404.07143v1.pdf)
- Hardware-aligned: [Native sparse attention](https://arxiv.org/abs/2502.11089)
- Context compression: [DeepSeek-OCR](https://arxiv.org/pdf/2510.18234), [hierarchical speculative decoding](https://arxiv.org/pdf/2404.11912v1.pdf)

### 🔍 Retrieval & Knowledge Systems
- RAG fundamentals: [REALM](https://arxiv.org/abs/2002.08909), [kNN-LM](https://arxiv.org/abs/1911.00172), [REFRAG](https://arxiv.org/pdf/2509.01092)
- Advanced retrieval: [Semantic IDs](https://arxiv.org/abs/2508.10478), [table retrieval](https://arxiv.org/pdf/2404.09889), multi-table systems
- Federated approaches: [Classifier-as-retriever with adapters](https://arxiv.org/pdf/2509.16508)

### 🧠 Reasoning, Alignment & Agents
- **Alignment**: [RLHF](https://arxiv.org/abs/1706.03741), [PPO](https://arxiv.org/abs/1707.06347), [InstructGPT](https://arxiv.org/abs/2203.02155), [Constitutional AI](https://arxiv.org/abs/2212.08073)
- **Reasoning**: [Hierarchical reasoning](https://arxiv.org/pdf/2506.21734), [recursive reasoning](https://arxiv.org/pdf/2510.04871), [reinforcement pre-training](https://arxiv.org/abs/2506.08007)
- **Agents**: [ReAct](https://arxiv.org/abs/2210.03629), [Reflexion](https://arxiv.org/abs/2303.11366), [Gato](https://arxiv.org/pdf/2205.06175), [DynaSaur](https://arxiv.org/abs/2411.01747)
- **Multi-agent**: [MAS-ZERO](https://arxiv.org/pdf/2505.14996) (zero-supervision design)
- **Efficiency**: [Small language models for agents](https://arxiv.org/abs/2506.02153)

### 🏛️ Novel Architectures & Theory
- Alternatives to transformers: [RWKV](https://arxiv.org/abs/2305.13048), [Mamba](https://arxiv.org/pdf/2409.15254), state-space models
- Novel approaches: [Kolmogorov-Arnold Networks (KAN)](https://arxiv.org/pdf/2404.19756), [U-Nets as belief propagation](https://arxiv.org/pdf/2404.18444)
- Theoretical foundations: [Neural Tangent Kernel](https://arxiv.org/abs/1806.07572), [manifold hypothesis](https://arxiv.org/abs/2504.01002)
- Training acceleration: [Neuron interaction networks](https://arxiv.org/pdf/2409.04434)

### 🔬 Interpretability & Analysis
- Attribution methods: [LIME](https://arxiv.org/abs/1602.04938), [Integrated Gradients](https://arxiv.org/abs/1703.01365), [SmoothGrad](https://arxiv.org/abs/1706.03825)
- Frameworks: [Rigorous interpretability science](https://arxiv.org/abs/1702.08608), [interpretation surveys](https://arxiv.org/abs/1706.07979)
- Evaluation: [TruthfulQA](https://arxiv.org/abs/2109.07958), [MLE-bench](https://arxiv.org/abs/2410.07095), [model calibration](https://arxiv.org/abs/1706.04599)
- Critical analysis: ["The Illusion of Thinking"](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf), [LLM evaluation challenges](https://arxiv.org/pdf/2502.07445)

### 🛡️ Security, Safety & Robustness
- **Alignment & Safety**: [InstructGPT](https://arxiv.org/abs/2203.02155), [Constitutional AI](https://arxiv.org/abs/2212.08073)
- **Security**: [Prompt sanitization](https://arxiv.org/abs/2504.05147), [MCP security](https://arxiv.org/pdf/2504.08623), [agentic AI self-defense](https://arxiv.org/pdf/2510.13825)
- **Threats**: [Cyber threat intelligence](https://arxiv.org/abs/2503.23175), reliability concerns

### 👁️ Computer Vision & Multimodal AI
- Vision transformers: [ViT](https://arxiv.org/abs/2010.11929), [CLIP](https://arxiv.org/abs/2103.00020), [SAM](https://arxiv.org/abs/2304.02643)
- Interpretability: [DeconvNet](https://arxiv.org/abs/1311.2901), [saliency maps](https://arxiv.org/abs/1312.6034), visualization techniques
- Segmentation: [All-CNN](https://arxiv.org/abs/1412.6806), [adversarial segmentation](https://arxiv.org/abs/1611.08408)
- Multimodal understanding: Vision-language models

### 🎯 Advanced Applications
- **Scientific AI**: [AI scientist](https://arxiv.org/pdf/2408.06292), [scientific software generation](https://arxiv.org/pdf/2509.06503), [architecture discovery](https://arxiv.org/pdf/2507.18074)
- **Specialized**: [Audio generation](https://arxiv.org/pdf/2407.14358) (Stable Audio), [molecular dynamics](https://arxiv.org/pdf/2405.07898)
- **Tabular**: [TabPFN](https://arxiv.org/pdf/2207.01848v3.pdf) for small classification problems
- **Test-time compute**: [o3 system](https://arxiv.org/pdf/2411.04996), [consciousness in AI](https://arxiv.org/pdf/2308.08708v3.pdf)

### 🎲 Probabilistic & Generative Models
- Probabilistic programming: [Data analysis](https://papers.nips.cc/paper/6060-a-probabilistic-programming-approach-to-probabilistic-data-analysis.pdf), [scene perception](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper.pdf) (Picture)
- MCMC methods: [Hamiltonian dynamics](https://arxiv.org/abs/1206.1901)
- Bayesian approaches: [Intuitive dynamics modeling](https://cocosci.berkeley.edu/tom/papers/collisions.pdf) 🔒

### ⚙️ Hardware & Systems
- Photonic computing for AI 🔒
- VLSI implementations: [Log-domain diffusion networks](https://papers.nips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf)
- Hardware-algorithm co-design

### 📜 Policy & Governance
- AI policy frameworks: [Dual-use models, open weights](https://www.ntia.gov/sites/default/files/publications/ntia-ai-open-model-report.pdf)
- Governance considerations for foundation models

---

## 🤝 Contributing

Have a paper that should be included? Found a broken link? Want to improve explanations?

**To suggest a paper:**
1. Check if it's already in [by-date.md](by-date.md)
2. Consider: Is it influential? Does it fit the learning path?
3. Open an issue with: Title, arXiv/URL, why it's important, suggested phase

**To fix issues:**
1. Broken links: Open an issue or PR with updated URL
2. Typos/improvements: PRs welcome!
3. Better explanations: Suggest edits to "Why" annotations

---

## 📚 Additional Resources

### Related Collections
- [Papers We Love](https://github.com/papers-we-love/papers-we-love) - Classic CS papers
- [Awesome Deep Learning Papers](https://github.com/terryum/awesome-deep-learning-papers) - DL fundamentals
- [ML Papers of The Week](https://github.com/dair-ai/ML-Papers-of-the-Week) - Weekly updates

### Tools & Platforms
- [arXiv](https://arxiv.org/) - Preprint repository
- [Papers With Code](https://paperswithcode.com/) - Papers + implementations
- [Semantic Scholar](https://www.semanticscholar.org/) - AI-powered paper search
- [Connected Papers](https://www.connectedpapers.com/) - Visual paper exploration

### Conference Deadlines
- 🎓 **[AI Deadlines](https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM,AP,KR,HCI)** - Track ML/AI conference submissions

---

## 📊 Repository Statistics

- **Total Papers**: 102+
- **Date Range**: 1997-2025
- **Coverage**: 13 major AI/ML topics
- **Open Access**: ~95% freely available
- **Structure**: 2 navigation modes (pedagogical + chronological)
- **Last Updated**: October 30, 2025

---

## 📄 License

This repository contains links to research papers. All papers remain under their original licenses and copyrights held by authors and publishers.

The curation, organization, and annotations in this repository are provided for educational purposes.

---

## 🙏 Acknowledgments

Papers compiled from:
- Major AI/ML conferences (NeurIPS, ICML, ICLR, CVPR, ACL, etc.)
- Leading research institutions and labs
- arXiv preprint server
- Open access initiatives

Special thanks to the researchers, authors, and institutions making their work freely available.

---

## 📬 Contact & Feedback

Found this helpful? Have suggestions? Want to discuss a paper?

- **Issues**: [Open an issue](../../issues) for bugs, suggestions, or paper recommendations
- **Discussions**: [Start a discussion](../../discussions) for paper analysis or learning questions

---

**Happy Reading! 📖🚀**

*Building knowledge, one paper at a time.*