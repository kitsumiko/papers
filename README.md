# papers
A small list of interesting papers.

# 2025.09
- [Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever](https://arxiv.org/pdf/2509.16508)
- [An AI system to help scientists write expert-level empirical software](https://arxiv.org/pdf/2509.06503)
- [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/pdf/2509.01092)

# 2025.08
- [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)

# 2025.07
- [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/pdf/2507.18074)

# 2025.06
- [Hierarchical Reasoning Model](https://arxiv.org/pdf/2506.21734)
- [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)

# 2025.05
- [MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision](https://arxiv.org/pdf/2505.14996)
- [How much do language models memorize?](https://arxiv.org/pdf/2505.24832)

# 2025.04
- [Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs](https://arxiv.org/abs/2504.05147)
- [Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies](https://arxiv.org/pdf/2504.08623)
- [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)

# 2025.03
- [Large Language Models are Unreliable for Cyber Threat Intelligence](https://arxiv.org/abs/2503.23175)

# 2025.02
- [Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon](https://arxiv.org/pdf/2502.07445)
- [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
- [The Illusion of Thinking](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

# 2024.11
- [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)
- [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009)
- [OpenAI o3 System Card](https://arxiv.org/pdf/2411.04996)

# 2024.10
- [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://arxiv.org/abs/2410.07095)

# 2024.09
- [Accelerating Training With Neuron Interaction And Nowcasting Networks](https://arxiv.org/pdf/2409.04434)
- [Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model](https://arxiv.org/pdf/2409.15254)
- [EuroLLM: Multilingual Language Models for Europe](https://arxiv.org/pdf/2409.11741)

# 2024.08
- [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/pdf/2408.06292)
    - [Dualscale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models](https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf)

# 2024.07
- [Dual-User Foundation Models with Widely Available Model Weights](https://www.ntia.gov/sites/default/files/publications/ntia-ai-open-model-report.pdf)
- [Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval](https://arxiv.org/pdf/2404.09889)
- [Stable Audio Open](https://arxiv.org/pdf/2407.14358)
- [The Llama 3 Herd of Models](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)

# 2024.06
- [A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration](https://proceedings.neurips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf)
- [Scalable MatMul-free Language Modeling](https://arxiv.org/pdf/2406.02528)

# 2024.05
- [Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System](https://arxiv.org/pdf/2405.07898)
- [KAN: Kolmogorov–Arnold Networks](https://arxiv.org/pdf/2404.19756)
- [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://arxiv.org/pdf/2404.18444)
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)
- [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/pdf/2402.03885)
- [Executable Code Actions Elicit Better LLM Agents](https://arxiv.org/pdf/2402.01030)

# 2024.04
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](https://arxiv.org/pdf/2404.11912v1.pdf)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143v1.pdf)
- [Efficient streaming language models with attention sinks](https://arxiv.org/pdf/2309.17453.pdf)
- [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/pdf/2308.08708v3.pdf)
- [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619)

# 2023
- [Bytes Are All You Need: Transformers Operating Directly On File Bytes](https://arxiv.org/pdf/2306.00238)
- [TabPFN: A transformer that solves small tabular classification problems in a second](https://arxiv.org/pdf/2207.01848v3.pdf)

# 2022
- [A Generalist Agent](https://arxiv.org/pdf/2205.06175)

# 2021
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)

# 2019-2020
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
- [High-dimensional on-chip dataflow sensing and routing using spatial photonic networks](https://www.nature.com/articles/s41566-023-01272-3.pdf)

# Deadlines
- [Academic Conferences](https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM,AP,KR,HCI)