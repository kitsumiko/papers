# Phase 1: Foundations (Start Here)

[← Back to Learning Path](../learning-path.md)

**Phase Overview**: This phase establishes the essential groundwork for understanding modern AI systems. You'll explore the fundamental principles of deep learning, from basic neural network architectures to the revolutionary transformer model that underpins nearly all current LLMs. By mastering these foundational concepts—including training dynamics, attention mechanisms, and the shift from convolutional to attention-based architectures—you'll build the technical vocabulary and intuition needed for everything that follows. Think of this as learning the alphabet before reading literature.

## 1.1 Deep Learning Basics
**Goal**: Understand the fundamental building blocks of modern deep learning

1. [Deep Learning (Nature Review)](https://www.nature.com/articles/nature14539) (LeCun, Bengio, Hinton, 2015)
   - *Why*: The canonical overview of deep learning principles - start here for the big picture
   
2. [Understanding the Difficulty of Training Deep Feedforward Neural Networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) (Glorot & Bengio, 2010)
   - *Why*: Introduces Xavier initialization and explains training challenges; foundational for understanding why deep networks are hard to train

3. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) (Ioffe & Szegedy, 2015)
   - *Why*: Essential technique for stable, fast training; enables much deeper networks

4. [Gradient-Based Learning Applied to Document Recognition (LeNet)](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (1997)
   - *Why*: Historical foundation of CNNs

5. [ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (2012)
   - *Why*: The paper that sparked the deep learning revolution
   
6. [Going Deeper with Convolutions (GoogLeNet)](https://arxiv.org/abs/1409.4842) (2014)
   - *Why*: Inception modules and efficient architecture design

7. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (2015)
   - *Why*: ResNets and skip connections - essential architecture innovation

## 1.2 Sequence Modeling & Recurrent Networks
**Goal**: Learn time-series and sequential data processing

1. [Long Short-Term Memory (LSTM)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) (1997)
   - *Why*: Foundation for sequence modeling

2. [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/pdf/2402.03885) (2024)
   - *Why*: Modern approach to time-series with foundation models

## 1.3 Generative Models
**Goal**: Understand how to generate new data

1. [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (2014)
   - *Why*: Revolutionary generative modeling approach

2. [Dualscale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models](https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf) (2024)
   - *Why*: Modern diffusion models for generative tasks

---

**Next**: [Phase 2: Large Language Models →](phase-02-llms.md)
