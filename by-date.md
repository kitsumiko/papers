# Papers by Publication Date

## 2025

### 2025.10
- [DeepSeek-OCR: Contexts Optical Compression](https://arxiv.org/pdf/2510.18234)
- [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/pdf/2510.13825)
- [The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton](https://arxiv.org/pdf/2510.09378)
- [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
- [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/pdf/2510.04871)

### 2025.09
- [Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever](https://arxiv.org/pdf/2509.16508)
- [An AI system to help scientists write expert-level empirical software](https://arxiv.org/pdf/2509.06503)
- [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/pdf/2509.01092)

### 2025.08
- [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)

### 2025.07
- [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/pdf/2507.18074)

### 2025.06
- [Hierarchical Reasoning Model](https://arxiv.org/pdf/2506.21734)
- [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
- [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)

### 2025.05
- [MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision](https://arxiv.org/pdf/2505.14996)
- [How much do language models memorize?](https://arxiv.org/pdf/2505.24832)

### 2025.04
- [Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs](https://arxiv.org/abs/2504.05147)
- [Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies](https://arxiv.org/pdf/2504.08623)
- [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)

### 2025.03
- [Large Language Models are Unreliable for Cyber Threat Intelligence](https://arxiv.org/abs/2503.23175)

### 2025.02
- [Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon](https://arxiv.org/pdf/2502.07445)
- [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
- [The Illusion of Thinking](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

## 2024

### 2024.11
- [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)
- [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009)
- [OpenAI o3 System Card](https://arxiv.org/pdf/2411.04996)

### 2024.10
- [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://arxiv.org/abs/2410.07095)

### 2024.09
- [Accelerating Training With Neuron Interaction And Nowcasting Networks](https://arxiv.org/pdf/2409.04434)
- [Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model](https://arxiv.org/pdf/2409.15254)
- [EuroLLM: Multilingual Language Models for Europe](https://arxiv.org/pdf/2409.11741)

### 2024.08
- [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/pdf/2408.06292)
- [Dualscale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models](https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf)

### 2024.07
- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)
- [Dual-User Foundation Models with Widely Available Model Weights](https://www.ntia.gov/sites/default/files/publications/ntia-ai-open-model-report.pdf)
- [Stable Audio Open](https://arxiv.org/pdf/2407.14358)

### 2024.06
- [Scalable MatMul-free Language Modeling](https://arxiv.org/pdf/2406.02528)
- [Understanding and Mitigating Tokenization Bias in Language Models](https://arxiv.org/abs/2406.16829)

### 2024.05
- [Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System](https://arxiv.org/pdf/2405.07898)

### 2024.04
- [KAN: Kolmogorov–Arnold Networks](https://arxiv.org/pdf/2404.19756)
- [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://arxiv.org/pdf/2404.18444)
- [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](https://arxiv.org/pdf/2404.11912v1.pdf)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143v1.pdf)
- [Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval](https://arxiv.org/pdf/2404.09889)

### 2024.03
- [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
- [Greed is All You Need: An Evaluation of Tokenizer Inference Methods](https://arxiv.org/abs/2403.01289)

### 2024.02
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)
- [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/pdf/2402.03885)
- [Executable Code Actions Elicit Better LLM Agents](https://arxiv.org/pdf/2402.01030)

## 2023

### 2023.09
- [Efficient streaming language models with attention sinks](https://arxiv.org/pdf/2309.17453.pdf)

### 2023.08
- [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/pdf/2308.08708v3.pdf)

### 2023.07
- [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621)

### 2023.05
- [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)

### 2023.03
- [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)

### 2023-Early
- [TabPFN: A transformer that solves small tabular classification problems in a second](https://arxiv.org/pdf/2207.01848v3.pdf)

## 2022

### 2022.12
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)

### 2022.10
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

### 2022.09
- [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)

### 2022.05
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [A Generalist Agent](https://arxiv.org/pdf/2205.06175)

### 2022.03
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)

## 2021

### 2021.04
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)

### 2021.03
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)

### 2021.02
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Vision Transformer/ViT)](https://arxiv.org/abs/2010.11929)

## 2020

### 2020.05
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)

### 2020.03
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)

### 2020.02
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)

## 2019

### 2019.11
- [Generalization through Memorization: Nearest Neighbor Language Models (kNN-LM)](https://arxiv.org/abs/1911.00172)

### 2019.10
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### 2019.09
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)

## 2018

- [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
- [SentencePiece: A simple and language independent subword tokenizer and detokenizer](https://arxiv.org/abs/1808.06226)

## 2017

### 2017.07
- [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/abs/1707.06347)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Methods for Interpreting and Understanding Deep Neural Networks](https://arxiv.org/abs/1706.07979)
- [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)
- [SmoothGrad: Removing Noise by Adding Noise](https://arxiv.org/abs/1706.03825)
- [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741)

### 2017.05
- [Learning How to Explain Neural Networks: PatternNet and PatternAttribution](https://arxiv.org/abs/1705.05598)

### 2017.03
- [Axiomatic Attribution for Deep Networks (Integrated Gradients)](https://arxiv.org/abs/1703.01365)
- [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)

### 2017.02
- [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608)
- [Visualizing Deep Neural Network Decisions: Prediction Difference Analysis](https://arxiv.org/abs/1702.04595)

## 2016

### 2016.12
- [Encapsulating Models and Approximate Inference Programs in Probabilistic Modules](https://arxiv.org/abs/1612.04759)
- [Measuring the Non-asymptotic Convergence of Sequential Monte Carlo Samplers using Probabilistic Programming](https://arxiv.org/abs/1612.02161)

### 2016.11
- [Semantic Segmentation using Adversarial Networks](https://arxiv.org/abs/1611.08408)
- [Time Series Structure Discovery via Probabilistic Program Synthesis](https://arxiv.org/abs/1611.07051)

### 2016.06
- [Learning Convolutional Neural Networks for Graphs](http://proceedings.mlr.press/v48/niepert16.pdf)
- [Synthesizing the Preferred Inputs for Neurons via Deep Generator Networks](https://arxiv.org/abs/1605.09304)

### 2016.02
- [LIME: "Why Should I Trust You?" Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)

### 2016-NeurIPS
- [A Probabilistic Programming Approach to Probabilistic Data Analysis](https://papers.nips.cc/paper/6060-a-probabilistic-programming-approach-to-probabilistic-data-analysis.pdf)

## 2015-2016

### 2016.08
- [Neural Machine Translation of Rare Words with Subword Units (BPE)](https://arxiv.org/abs/1508.07909)

### 2015.12
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
- [Deep Taylor Decomposition: Explaining Nonlinear Classification Decisions](https://arxiv.org/abs/1512.02479)

### 2015.05
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

### 2015-CVPR
- [Picture: An Imperative Probabilistic Programming Language for Scene Perception](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper.pdf)

### 2015-Earlier
- 🔒 [Deep Learning (Nature Review)](https://www.nature.com/articles/nature14539) - *Paywalled*
- [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)

## 2014

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/)
- [Going Deeper with Convolutions (GoogLeNet)](https://arxiv.org/abs/1409.4842)
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)

## 2013

- [Efficient Estimation of Word Representations in Vector Space (Word2Vec)](https://arxiv.org/abs/1301.3781)
- [Visualizing and Understanding Convolutional Networks (DeconvNet)](https://arxiv.org/abs/1311.2901)
- [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)
- [Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs](http://papers.nips.cc/paper/4881-approximate-bayesian-image-interpretation-using-generative-probabilistic-graphics-programs.pdf)

## 2012

- [ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [MCMC using Hamiltonian dynamics](https://arxiv.org/abs/1206.1901)

## 2010

- [Understanding the Difficulty of Training Deep Feedforward Neural Networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- [A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration](https://papers.nips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf)

## 2009

- 🔒 [A Bayesian Framework for Modeling Intuitive Dynamics](https://cocosci.berkeley.edu/tom/papers/collisions.pdf) - *May require institutional access*

## 1997-2001

- 🔒 [Long Short-Term Memory (LSTM)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) (1997) - *Paywalled*
- [Gradient-Based Learning Applied to Document Recognition (LeNet)](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (1998)

---

## Additional Resources

### Papers Not Yet in Learning Path
- [Bytes Are All You Need: Transformers Operating Directly On File Bytes](https://arxiv.org/pdf/2306.00238) (2023)
- 🔒 [High-dimensional on-chip dataflow sensing and routing using spatial photonic networks](https://www.nature.com/articles/s41566-023-01272-3.pdf) (2023) - *Paywalled*
- [Deep Taylor Decomposition: Explaining Nonlinear Classification Decisions](https://arxiv.org/abs/1512.02479) (2015)
- [Learning How to Explain Neural Networks: PatternNet and PatternAttribution](https://arxiv.org/abs/1705.05598) (2017)
- [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) (2017)
- [Encapsulating Models and Approximate Inference Programs in Probabilistic Modules](https://arxiv.org/abs/1612.04759) (2016)
- [Measuring the Non-asymptotic Convergence of Sequential Monte Carlo Samplers using Probabilistic Programming](https://arxiv.org/abs/1612.02161) (2016)
- [Time Series Structure Discovery via Probabilistic Program Synthesis](https://arxiv.org/abs/1611.07051) (2016)
- [Learning Convolutional Neural Networks for Graphs](http://proceedings.mlr.press/v48/niepert16.pdf) (2016)
- [Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs](http://papers.nips.cc/paper/4881-approximate-bayesian-image-interpretation-using-generative-probabilistic-graphics-programs.pdf) (2013)

### Deadlines
- [Academic Conferences](https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM,AP,KR,HCI)

---

**Total Papers in Learning Path**: 101 papers  
**Paywalled Papers**: 4 (marked with 🔒)  
**Open Access**: ~96%

[← Back to Main](README.md) | [→ View Learning Path](learning-path.md)