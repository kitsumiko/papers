# Papers by Publication Date

## 2025

### 2025.12
- [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/abs/2512.24880)
- üìÑ [New York State A6453A/S6953B: Training and Use of Artificial Intelligence Frontier Models](https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A) (Signed December 2025)

### 2025.11
- [SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks](https://openreview.net/pdf?id=QQhQIqons0)
- [Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning](https://arxiv.org/pdf/2511.22226)
- [Weight-sparse transformers have interpretable circuits](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf) (OpenAI, Nov 13, 2025)
- [Nested Learning: The Illusion of Deep Learning Architectures](https://abehrouz.github.io/files/NL.pdf) - NeurIPS 2025

### 2025.10
- [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
- [DeepSeek-OCR: Contexts Optical Compression](https://arxiv.org/pdf/2510.18234)
- [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/pdf/2510.13825)
- [The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton](https://arxiv.org/pdf/2510.09378)
- [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
- [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/pdf/2510.04871)

### 2025.09
- [Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis](https://arxiv.org/pdf/2509.15392)
- [Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever](https://arxiv.org/pdf/2509.16508)
- [An AI system to help scientists write expert-level empirical software](https://arxiv.org/pdf/2509.06503)
- [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/pdf/2509.01092)

### 2025.08
- [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)

### 2025.07
- [Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data](https://arxiv.org/pdf/2507.14805) (Cloud et al., 2025)
- [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/pdf/2507.18074)

### 2025.06
- [Hierarchical Reasoning Model](https://arxiv.org/pdf/2506.21734)
- [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
- [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)

### 2025.05
- [Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/pdf/2505.17638)
- [MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision](https://arxiv.org/pdf/2505.14996)
- [How much do language models memorize?](https://arxiv.org/pdf/2505.24832)

### 2025.04
- [Pr$ŒµŒµ$mpt: Sanitizing Sensitive Prompts for LLMs](https://arxiv.org/abs/2504.05147)
- [Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies](https://arxiv.org/pdf/2504.08623)
- [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)

### 2025.03
- [Large Language Models are Unreliable for Cyber Threat Intelligence](https://arxiv.org/abs/2503.23175)

### 2025.02
- [Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon](https://arxiv.org/pdf/2502.07445)
- [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
- [The Illusion of Thinking](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

## 2024

### 2024.11
- [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)
- [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009)
- [OpenAI o3 System Card](https://arxiv.org/pdf/2411.04996)

### 2024.10
- [Automated Red Teaming with GOAT: the Generative Offensive Agent Tester](https://arxiv.org/abs/2410.01606)
- [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://arxiv.org/abs/2410.07095)

### 2024.09
- [Accelerating Training With Neuron Interaction And Nowcasting Networks](https://arxiv.org/pdf/2409.04434)
- [Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model](https://arxiv.org/pdf/2409.15254)
- [EuroLLM: Multilingual Language Models for Europe](https://arxiv.org/pdf/2409.11741)

### 2024.08
- [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/pdf/2408.06292)
- [Dualscale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models](https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf)

### 2024.07
- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)
- [Dual-User Foundation Models with Widely Available Model Weights](https://www.ntia.gov/sites/default/files/publications/ntia-ai-open-model-report.pdf)
- [Stable Audio Open](https://arxiv.org/pdf/2407.14358)

### 2024.06
- [Scalable MatMul-free Language Modeling](https://arxiv.org/pdf/2406.02528)
- [Understanding and Mitigating Tokenization Bias in Language Models](https://arxiv.org/abs/2406.16829)
- [WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](https://arxiv.org/abs/2406.18495)

### 2024.05
- [Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System](https://arxiv.org/pdf/2405.07898)
- [Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems](https://arxiv.org/abs/2405.06624)

### 2024.04
- [KAN: Kolmogorov‚ÄìArnold Networks](https://arxiv.org/pdf/2404.19756)
- [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://arxiv.org/pdf/2404.18444)
- [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](https://arxiv.org/pdf/2404.11912v1.pdf)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143v1.pdf)
- [Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval](https://arxiv.org/pdf/2404.09889)

### 2024.03
- [Greed is All You Need: An Evaluation of Tokenizer Inference Methods](https://arxiv.org/abs/2403.01289)
- [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218)

### 2024.02
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)
- [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/pdf/2402.03885)
- [Executable Code Actions Elicit Better LLM Agents](https://arxiv.org/pdf/2402.01030)
- [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)

### 2024.01
- [Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training](https://arxiv.org/abs/2401.05566)

## 2023

### 2023.04
- [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
- [Visual Instruction Tuning (LLaVA)](https://arxiv.org/abs/2304.08485)

### 2023.12
- [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390)
- [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)

### 2023.09
- [GPTFuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/pdf/2309.10253)
- [Efficient streaming language models with attention sinks](https://arxiv.org/pdf/2309.17453.pdf)

### 2023.08
- [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/pdf/2308.08708v3.pdf)

### 2023.07
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)

### 2023.05
- [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)

### 2023.03
- [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)

### 2023-Early
- [TabPFN: A transformer that solves small tabular classification problems in a second](https://arxiv.org/pdf/2207.01848v3.pdf)

## 2022

### 2022.12
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)

### 2022.09
- [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)

### 2022.11
- [Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102)

### 2022.10
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

### 2022
- [Scalable Oversight](https://www.anthropic.com/research/scalable-oversight) (Anthropic)
- [Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) (Anthropic)

### 2022.05
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [A Generalist Agent](https://arxiv.org/pdf/2205.06175)

### 2022.03
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)

### 2021.09
- [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)

## 2021

### 2021.04
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)

### 2021.03
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)

### 2021.02
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Vision Transformer/ViT)](https://arxiv.org/abs/2010.11929)

### 2021.01
- [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)

## 2020

### 2020.09
- [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)

### 2020.05
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

### 2020.04
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

### 2020.03
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)

### 2020.02
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)

### 2020.01
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

## 2019

### 2019.11
- [Generalization through Memorization: Nearest Neighbor Language Models (kNN-LM)](https://arxiv.org/abs/1911.00172)

### 2019.10
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### 2019.09
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)

### 2018.02
- [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a.html)

## 2018

### 2018.11
- [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965)

### 2018.06
- [Relational Recurrent Neural Networks](https://arxiv.org/abs/1806.01822)
- [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)

### 2018.08
- [SentencePiece: A simple and language independent subword tokenizer and detokenizer](https://arxiv.org/abs/1808.06226)

## 2017

### 2017.07
- [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/abs/1707.06347)
- [A Simple Neural Network Module for Relational Reasoning](https://arxiv.org/abs/1706.01427)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Methods for Interpreting and Understanding Deep Neural Networks](https://arxiv.org/abs/1706.07979)
- [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)
- [SmoothGrad: Removing Noise by Adding Noise](https://arxiv.org/abs/1706.03825)
- [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741)

### 2017.05
- [Learning How to Explain Neural Networks: PatternNet and PatternAttribution](https://arxiv.org/abs/1705.05598)

### 2017.03
- [Axiomatic Attribution for Deep Networks (Integrated Gradients)](https://arxiv.org/abs/1703.01365)
- [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)

### 2017.02
- [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608)
- [Visualizing Deep Neural Network Decisions: Prediction Difference Analysis](https://arxiv.org/abs/1702.04595)

### 2017.11
- [AI Safety Gridworlds](https://arxiv.org/abs/1711.09883)

## 2016

### 2016.12
- [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595)
- [Encapsulating Models and Approximate Inference Programs in Probabilistic Modules](https://arxiv.org/abs/1612.04759)
- [Measuring the Non-asymptotic Convergence of Sequential Monte Carlo Samplers using Probabilistic Programming](https://arxiv.org/abs/1612.02161)

### 2016.11
- [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731)
- [Semantic Segmentation using Adversarial Networks](https://arxiv.org/abs/1611.08408)
- [Time Series Structure Discovery via Probabilistic Program Synthesis](https://arxiv.org/abs/1611.07051)

### 2016.03
- [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)

### 2016.10
- [Equality of Opportunity in Supervised Learning](https://arxiv.org/abs/1610.02413)

### 2016.06
- [Learning Convolutional Neural Networks for Graphs](http://proceedings.mlr.press/v48/niepert16.pdf)
- [Synthesizing the Preferred Inputs for Neurons via Deep Generator Networks](https://arxiv.org/abs/1605.09304)
- [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)

### 2016.02
- [LIME: "Why Should I Trust You?" Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)

### 2016-NeurIPS
- [A Probabilistic Programming Approach to Probabilistic Data Analysis](https://papers.nips.cc/paper/6060-a-probabilistic-programming-approach-to-probabilistic-data-analysis.pdf)

## 2015-2016

### 2016.08
- [Neural Machine Translation of Rare Words with Subword Units (BPE)](https://arxiv.org/abs/1508.07909)

### 2015.12
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
- [Deep Taylor Decomposition: Explaining Nonlinear Classification Decisions](https://arxiv.org/abs/1512.02479)

### 2015.11
- [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391)
- [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)

### 2015.06
- [Pointer Networks](https://arxiv.org/abs/1506.03134)

### 2015.05
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

### 2015-CVPR
- [Picture: An Imperative Probabilistic Programming Language for Scene Perception](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper.pdf)

### 2015-Earlier
- üîí [Deep Learning (Nature Review)](https://www.nature.com/articles/nature14539) - *Paywalled*
- [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)

## 2014

### 2014.10
- [Neural Turing Machines](https://arxiv.org/abs/1410.5401)

### 2014.09
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Going Deeper with Convolutions (GoogLeNet)](https://arxiv.org/abs/1409.4842)

### 2014.06
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)

### 2014.01
- [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/)

## 2013

- [Efficient Estimation of Word Representations in Vector Space (Word2Vec)](https://arxiv.org/abs/1301.3781)
- [Visualizing and Understanding Convolutional Networks (DeconvNet)](https://arxiv.org/abs/1311.2901)
- [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)
- [Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs](http://papers.nips.cc/paper/4881-approximate-bayesian-image-interpretation-using-generative-probabilistic-graphics-programs.pdf)

## 2012

- [ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [MCMC using Hamiltonian dynamics](https://arxiv.org/abs/1206.1901)

## 2010

- [Understanding the Difficulty of Training Deep Feedforward Neural Networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- [A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration](https://papers.nips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf)

## 2009

- üîí [A Bayesian Framework for Modeling Intuitive Dynamics](https://cocosci.berkeley.edu/tom/papers/collisions.pdf) - *May require institutional access*

## 1997-2001

- üîí [Long Short-Term Memory (LSTM)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) (1997) - *Paywalled*
- [Gradient-Based Learning Applied to Document Recognition (LeNet)](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (1998)

---

**Total Papers in Learning Path**: 150+ papers
**Paywalled Papers**: 4 (marked with üîí)  
**Open Access**: ~97%

[‚Üê Back to Main](README.md) | [‚Üí View Learning Path](learning-path.md)